{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "catholic-wrapping",
   "metadata": {
    "papermill": {
     "duration": 0.018431,
     "end_time": "2021-06-26T01:17:38.369831",
     "exception": false,
     "start_time": "2021-06-26T01:17:38.351400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test set inference notebook for 2nd place solution\n",
    "\n",
    "- Refer to the discussion post [2nd place solution overview](https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/248296) for an explanation of the approach\n",
    "- @leecming if you have any questions about my approach\n",
    "- Other than the competition dataset, the only external dataset used is a [HuggingFace fine-tuned model/tokenizer](https://www.kaggle.com/leecming/robertalabelclassifierrawipcc)\n",
    "\n",
    "Broadly, the notebook does the following :-\n",
    "1. Search all test documents for strings in the form \"LONG-NAME (ACRONYM)\" and create mappings of documents to these LONG-NAMES/ACRONYMS\n",
    "   - Accept acronyms only if they're longer than 3 characters \n",
    "2. Classify LONG-NAMES as datasets using a fine-tuned HuggingFace Transformer binary classifier \n",
    "   - Accept candidates only if they have probability above MIN_PROB\n",
    "3. Generate a dynamic lookup table for LONG-NAMES that exceed HIGH_FREQ document frequency + definite labels from the training dataset\n",
    "4. Search the cleaned text of all documents for the LONG-NAMES from step 3. and generate a combined ID-to-candidate table\n",
    "5. Collate a final set of predictions \n",
    "  - Remove candidates too similar to definite training labels using the fuzzywuzzy string comparison functionality\n",
    "  - Accept a candidate if either they exceed HIGH_FREQ doc frequency OR regex match ([A-Z][a-z]+ )+(Study|Survey)$ OR regex match (Study|Survey) of\n",
    "  - Add a candidate's acronym if it is present in the RAW text \n",
    "  - For any document that has no suitable candidates, predict empty string\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-award",
   "metadata": {
    "_cell_guid": "90464a5c-caea-483e-93f1-621ec102c9f2",
    "_uuid": "f5b40245-9676-4b76-af60-0cc92668fc3e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:38.415792Z",
     "iopub.status.busy": "2021-06-26T01:17:38.415248Z",
     "iopub.status.idle": "2021-06-26T01:17:42.430044Z",
     "shell.execute_reply": "2021-06-26T01:17:42.429452Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.597635Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.042969,
     "end_time": "2021-06-26T01:17:42.430194",
     "exception": false,
     "start_time": "2021-06-26T01:17:38.387225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-uganda",
   "metadata": {
    "papermill": {
     "duration": 0.016569,
     "end_time": "2021-06-26T01:17:42.463698",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.447129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model settings and hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "independent-compact",
   "metadata": {
    "_cell_guid": "41f47c3e-d73a-4781-a5e0-7fdb8b88c49a",
    "_uuid": "6d833ff6-e8c3-4386-b098-b35e2349dc73",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:42.503140Z",
     "iopub.status.busy": "2021-06-26T01:17:42.501889Z",
     "iopub.status.idle": "2021-06-26T01:17:42.504195Z",
     "shell.execute_reply": "2021-06-26T01:17:42.504646Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.606426Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024547,
     "end_time": "2021-06-26T01:17:42.504780",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.480233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON_FEATURE_DIR = '../input/coleridgeinitiative-show-us-the-data/test/' # Path to the competition test-set\n",
    "PRETRAINED_CLASSIFIER = '../input/robertalabelclassifierrawipcc/' # Path to HuggingFace model/tokenizer; also found at https://www.kaggle.com/leecming/robertalabelclassifierrawipcc\n",
    "MIN_PROB = 0.9 # Threshold probability for accepting a dataset label prediction\n",
    "HIGH_FREQ = 50  # Threshold document frequency (i.e., # of docs that contain the candidate string)\n",
    "MATCHING_THRESHOLD = 90  # Threshold similarity for removing candidate strings too similar to definite training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nervous-rwanda",
   "metadata": {
    "_cell_guid": "0dfe8298-1fc8-4bd9-aee7-2aeb8fb77be1",
    "_uuid": "d12cfe51-09eb-45f1-9e37-4e5addd699f3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:42.544272Z",
     "iopub.status.busy": "2021-06-26T01:17:42.543585Z",
     "iopub.status.idle": "2021-06-26T01:17:42.547675Z",
     "shell.execute_reply": "2021-06-26T01:17:42.547244Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.619776Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026594,
     "end_time": "2021-06-26T01:17:42.547875",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.521281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing text cleaning code provided by organizers\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-holiday",
   "metadata": {
    "papermill": {
     "duration": 0.018106,
     "end_time": "2021-06-26T01:17:42.586349",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.568243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Schwartz-Hearst code \n",
    "\n",
    "Schwartz-Hearst algorithm as described in [A Simple Algorithm for Identifying Abbreviation Definitions in Biomedical Text](https://psb.stanford.edu/psb-online/proceedings/psb03/schwartz.pdf) and Python code adapted from [here](https://github.com/philgooch/abbreviation-extraction)\n",
    "\n",
    "The long code chunk below is hidden for readability but can be interpreted as an advanced regex search algorithm used to search for strings in the form of LONG-NAME (ACRONYM) e.g., \"Baltimore Longitudinal Study of Aging (BLSA)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "headed-steam",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:42.649641Z",
     "iopub.status.busy": "2021-06-26T01:17:42.648726Z",
     "iopub.status.idle": "2021-06-26T01:17:42.659537Z",
     "shell.execute_reply": "2021-06-26T01:17:42.659921Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.715621Z"
    },
    "papermill": {
     "duration": 0.05665,
     "end_time": "2021-06-26T01:17:42.660047",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.603397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Candidate(str):\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.start = 0\n",
    "        self.stop = 0\n",
    "\n",
    "    def set_position(self, start, stop):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "\n",
    "\n",
    "def yield_lines_from_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line = line.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                line = line.decode('latin-1').encode('utf-8').decode('utf-8')\n",
    "            line = line.strip()\n",
    "            yield line\n",
    "\n",
    "\n",
    "def yield_lines_from_doc(doc_text):\n",
    "    for line in doc_text.split(\"\\n\"):\n",
    "        yield line.strip()\n",
    "\n",
    "\n",
    "def best_candidates(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence: line read from input file\n",
    "    :return: a Candidate iterator\n",
    "    \"\"\"\n",
    "\n",
    "    if '(' in sentence:\n",
    "        # Check some things first\n",
    "        if sentence.count('(') != sentence.count(')'):\n",
    "            raise ValueError(\"Unbalanced parentheses: {}\".format(sentence))\n",
    "\n",
    "        if sentence.find('(') > sentence.find(')'):\n",
    "            raise ValueError(\"First parentheses is right: {}\".format(sentence))\n",
    "\n",
    "        close_index = -1\n",
    "        while 1:\n",
    "            # Look for open parenthesis. Need leading whitespace to avoid matching mathematical and chemical formulae\n",
    "            open_index = sentence.find(' (', close_index + 1)\n",
    "\n",
    "            if open_index == -1: break\n",
    "\n",
    "            # Advance beyond whitespace\n",
    "            open_index += 1\n",
    "\n",
    "            # Look for closing parentheses\n",
    "            close_index = open_index + 1\n",
    "            open_count = 1\n",
    "            skip = False\n",
    "            while open_count:\n",
    "                try:\n",
    "                    char = sentence[close_index]\n",
    "                except IndexError:\n",
    "                    # We found an opening bracket but no associated closing bracket\n",
    "                    # Skip the opening bracket\n",
    "                    skip = True\n",
    "                    break\n",
    "                if char == '(':\n",
    "                    open_count += 1\n",
    "                elif char in [')', ';', ':']:\n",
    "                    open_count -= 1\n",
    "                close_index += 1\n",
    "\n",
    "            if skip:\n",
    "                close_index = open_index + 1\n",
    "                continue\n",
    "\n",
    "            # Output if conditions are met\n",
    "            start = open_index + 1\n",
    "            stop = close_index - 1\n",
    "            candidate = sentence[start:stop]\n",
    "\n",
    "            # Take into account whitespace that should be removed\n",
    "            start = start + len(candidate) - len(candidate.lstrip())\n",
    "            stop = stop - len(candidate) + len(candidate.rstrip())\n",
    "            candidate = sentence[start:stop]\n",
    "\n",
    "            if conditions(candidate):\n",
    "                new_candidate = Candidate(candidate)\n",
    "                new_candidate.set_position(start, stop)\n",
    "                yield new_candidate\n",
    "\n",
    "\n",
    "def conditions(candidate):\n",
    "    \"\"\"\n",
    "    Based on Schwartz&Hearst\n",
    "\n",
    "    2 <= len(str) <= 10\n",
    "    len(tokens) <= 2\n",
    "    re.search(r'\\p{L}', str)\n",
    "    str[0].isalnum()\n",
    "\n",
    "    and extra:\n",
    "    if it matches (\\p{L}\\.?\\s?){2,}\n",
    "    it is a good candidate.\n",
    "\n",
    "    :param candidate: candidate abbreviation\n",
    "    :return: True if this is a good candidate\n",
    "    \"\"\"\n",
    "    viable = True\n",
    "    if regex.match(r'(\\p{L}\\.?\\s?){2,}', candidate.lstrip()):\n",
    "        viable = True\n",
    "    if len(candidate) < 2 or len(candidate) > 10:\n",
    "        viable = False\n",
    "    if len(candidate.split()) > 2:\n",
    "        viable = False\n",
    "    if not regex.search(r'\\p{L}', candidate):\n",
    "        viable = False\n",
    "    if not candidate[0].isalnum():\n",
    "        viable = False\n",
    "\n",
    "    return viable\n",
    "\n",
    "\n",
    "def get_definition(candidate, sentence):\n",
    "    \"\"\"\n",
    "    Takes a candidate and a sentence and returns the definition candidate.\n",
    "\n",
    "    The definition candidate is the set of tokens (in front of the candidate)\n",
    "    that starts with a token starting with the first character of the candidate\n",
    "\n",
    "    :param candidate: candidate abbreviation\n",
    "    :param sentence: current sentence (single line from input file)\n",
    "    :return: candidate definition for this abbreviation\n",
    "    \"\"\"\n",
    "    # Take the tokens in front of the candidate\n",
    "    tokens = regex.split(r'[\\s\\-]+', sentence[:candidate.start - 2].lower())\n",
    "    # the char that we are looking for\n",
    "    key = candidate[0].lower()\n",
    "\n",
    "    # Count the number of tokens that start with the same character as the candidate\n",
    "    first_chars = [t[0] for t in filter(None, tokens)]\n",
    "\n",
    "    definition_freq = first_chars.count(key)\n",
    "    candidate_freq = candidate.lower().count(key)\n",
    "\n",
    "    # Look for the list of tokens in front of candidate that\n",
    "    # have a sufficient number of tokens starting with key\n",
    "    if candidate_freq <= definition_freq:\n",
    "        # we should at least have a good number of starts\n",
    "        count = 0\n",
    "        start = 0\n",
    "        start_index = len(first_chars) - 1\n",
    "        while count < candidate_freq:\n",
    "            if abs(start) > len(first_chars):\n",
    "                raise ValueError(\"candidate {} not found\".format(candidate))\n",
    "            start -= 1\n",
    "            # Look up key in the definition\n",
    "            try:\n",
    "                start_index = first_chars.index(key, len(first_chars) + start)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "            # Count the number of keys in definition\n",
    "            count = first_chars[start_index:].count(key)\n",
    "\n",
    "        # We found enough keys in the definition so return the definition as a definition candidate\n",
    "        start = len(' '.join(tokens[:start_index]))\n",
    "        stop = candidate.start - 1\n",
    "        candidate = sentence[start:stop]\n",
    "\n",
    "        # Remove whitespace\n",
    "        start = start + len(candidate) - len(candidate.lstrip())\n",
    "        stop = stop - len(candidate) + len(candidate.rstrip())\n",
    "        candidate = sentence[start:stop]\n",
    "\n",
    "        new_candidate = Candidate(candidate)\n",
    "        new_candidate.set_position(start, stop)\n",
    "        return new_candidate\n",
    "\n",
    "    else:\n",
    "        raise ValueError('There are less keys in the tokens in front of candidate than there are in the candidate')\n",
    "\n",
    "\n",
    "def select_definition(definition, abbrev):\n",
    "    \"\"\"\n",
    "    Takes a definition candidate and an abbreviation candidate\n",
    "    and returns True if the chars in the abbreviation occur in the definition\n",
    "\n",
    "    Based on\n",
    "    A simple algorithm for identifying abbreviation definitions in biomedical texts, Schwartz & Hearst\n",
    "    :param definition: candidate definition\n",
    "    :param abbrev: candidate abbreviation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(definition) < len(abbrev):\n",
    "        raise ValueError('Abbreviation is longer than definition')\n",
    "\n",
    "    if abbrev in definition.split():\n",
    "        raise ValueError('Abbreviation is full word of definition')\n",
    "\n",
    "    s_index = -1\n",
    "    l_index = -1\n",
    "\n",
    "    while 1:\n",
    "        try:\n",
    "            long_char = definition[l_index].lower()\n",
    "        except IndexError:\n",
    "            raise\n",
    "\n",
    "        short_char = abbrev[s_index].lower()\n",
    "\n",
    "        if not short_char.isalnum():\n",
    "            s_index -= 1\n",
    "\n",
    "        if s_index == -1 * len(abbrev):\n",
    "            if short_char == long_char:\n",
    "                if l_index == -1 * len(definition) or not definition[l_index - 1].isalnum():\n",
    "                    break\n",
    "                else:\n",
    "                    l_index -= 1\n",
    "            else:\n",
    "                l_index -= 1\n",
    "                if l_index == -1 * (len(definition) + 1):\n",
    "                    raise ValueError(\"definition {} was not found in {}\".format(abbrev, definition))\n",
    "\n",
    "        else:\n",
    "            if short_char == long_char:\n",
    "                s_index -= 1\n",
    "                l_index -= 1\n",
    "            else:\n",
    "                l_index -= 1\n",
    "\n",
    "    new_candidate = Candidate(definition[l_index:len(definition)])\n",
    "    new_candidate.set_position(definition.start, definition.stop)\n",
    "    definition = new_candidate\n",
    "\n",
    "    tokens = len(definition.split())\n",
    "    length = len(abbrev)\n",
    "\n",
    "    if tokens > min([length + 5, length * 2]):\n",
    "        raise ValueError(\"did not meet min(|A|+5, |A|*2) constraint\")\n",
    "\n",
    "    # Do not return definitions that contain unbalanced parentheses\n",
    "    if definition.count('(') != definition.count(')'):\n",
    "        raise ValueError(\"Unbalanced parentheses not allowed in a definition\")\n",
    "\n",
    "    return definition\n",
    "\n",
    "\n",
    "def extract_abbreviation_definition_pairs(file_path=None,\n",
    "                                          doc_text=None,\n",
    "                                          most_common_definition=False,\n",
    "                                          first_definition=False):\n",
    "    abbrev_map = dict()\n",
    "    list_abbrev_map = defaultdict(list)\n",
    "    counter_abbrev_map = dict()\n",
    "    omit = 0\n",
    "    written = 0\n",
    "    if file_path:\n",
    "        sentence_iterator = enumerate(yield_lines_from_file(file_path))\n",
    "    elif doc_text:\n",
    "        sentence_iterator = enumerate(yield_lines_from_doc(doc_text))\n",
    "    else:\n",
    "        return abbrev_map\n",
    "\n",
    "    collect_definitions = False\n",
    "    if most_common_definition or first_definition:\n",
    "        collect_definitions = True\n",
    "\n",
    "    for i, sentence in sentence_iterator:\n",
    "        # Remove any quotes around potential candidate terms\n",
    "        clean_sentence = regex.sub(r'([(])[\\'\"\\p{Pi}]|[\\'\"\\p{Pf}]([);:])', r'\\1\\2', sentence)\n",
    "        try:\n",
    "            for candidate in best_candidates(clean_sentence):\n",
    "                try:\n",
    "                    definition = get_definition(candidate, clean_sentence)\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    log.debug(\"{} Omitting candidate {}. Reason: {}\".format(i, candidate, e.args[0]))\n",
    "                    omit += 1\n",
    "                else:\n",
    "                    try:\n",
    "                        definition = select_definition(definition, candidate)\n",
    "                    except (ValueError, IndexError) as e:\n",
    "                        log.debug(\"{} Omitting definition {} for candidate {}. Reason: {}\".format(i, definition, candidate, e.args[0]))\n",
    "                        omit += 1\n",
    "                    else:\n",
    "                        # Either append the current definition to the list of previous definitions ...\n",
    "                        if collect_definitions:\n",
    "                            list_abbrev_map[candidate].append(definition)\n",
    "                        else:\n",
    "                            # Or update the abbreviations map with the current definition\n",
    "                            abbrev_map[candidate] = definition\n",
    "                        written += 1\n",
    "        except (ValueError, IndexError) as e:\n",
    "            log.debug(\"{} Error processing sentence {}: {}\".format(i, sentence, e.args[0]))\n",
    "    log.debug(\"{} abbreviations detected and kept ({} omitted)\".format(written, omit))\n",
    "\n",
    "    # Return most common definition for each term\n",
    "    if collect_definitions:\n",
    "        if most_common_definition:\n",
    "            # Return the most common definition for each term\n",
    "            for k,v in list_abbrev_map.items():\n",
    "                counter_abbrev_map[k] = Counter(v).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Return the first definition for each term\n",
    "            for k, v in list_abbrev_map.items():\n",
    "                counter_abbrev_map[k] = v[0]\n",
    "        return counter_abbrev_map\n",
    "\n",
    "    # Or return the last encountered definition for each term\n",
    "    return abbrev_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-albany",
   "metadata": {
    "papermill": {
     "duration": 0.01669,
     "end_time": "2021-06-26T01:17:42.693028",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.676338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset binary classifier \n",
    "Self-contained code to classify a list of input strings (as datasets or not); returns a list of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "laden-raleigh",
   "metadata": {
    "_cell_guid": "fb267b87-2505-4b6a-8196-3f86470dd74c",
    "_uuid": "719d3eae-d82c-4e63-8c83-b77ec3791d1a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:42.733261Z",
     "iopub.status.busy": "2021-06-26T01:17:42.732505Z",
     "iopub.status.idle": "2021-06-26T01:17:42.735390Z",
     "shell.execute_reply": "2021-06-26T01:17:42.734847Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.757155Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025923,
     "end_time": "2021-06-26T01:17:42.735502",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.709579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_labels(string_list):\n",
    "    classifier = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_CLASSIFIER).half().cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_CLASSIFIER)\n",
    "    classifier.eval()\n",
    "\n",
    "    label_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx_start in range(0, len(string_list), 64):\n",
    "            batch_idx_end = min(batch_idx_start + 64, len(string_list))\n",
    "            current_batch = string_list[batch_idx_start:batch_idx_end]\n",
    "            batch_features = tokenizer(current_batch,\n",
    "                                       truncation=True,\n",
    "                                       max_length=64,\n",
    "                                       padding='max_length',\n",
    "                                       add_special_tokens=True,\n",
    "                                       return_tensors='pt')\n",
    "            batch_features = {k: v.cuda() for k,v in batch_features.items()}\n",
    "            model_output = classifier(**batch_features,\n",
    "                                      return_dict=True)\n",
    "            batch_preds = torch.nn.Softmax(-1)(model_output['logits'])[:, 1].cpu().numpy()\n",
    "            label_preds.append(batch_preds)\n",
    "            \n",
    "    return np.concatenate(label_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-johnson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T00:59:53.454071Z",
     "iopub.status.busy": "2021-06-26T00:59:53.453715Z",
     "iopub.status.idle": "2021-06-26T00:59:53.461300Z",
     "shell.execute_reply": "2021-06-26T00:59:53.459925Z",
     "shell.execute_reply.started": "2021-06-26T00:59:53.454036Z"
    },
    "papermill": {
     "duration": 0.017956,
     "end_time": "2021-06-26T01:17:42.770137",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.752181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document text search \n",
    "Load a given document's json file, tokenize into sentences, and search for LONG-NAME (ACRONYMS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "absolute-brunei",
   "metadata": {
    "_cell_guid": "f2d7a14e-e835-4a77-b4d4-7693a5648488",
    "_uuid": "e827ac7d-9105-4d11-b68b-4e280e935b36",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:42.808512Z",
     "iopub.status.busy": "2021-06-26T01:17:42.807862Z",
     "iopub.status.idle": "2021-06-26T01:17:42.810164Z",
     "shell.execute_reply": "2021-06-26T01:17:42.810561Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.772340Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023932,
     "end_time": "2021-06-26T01:17:42.810676",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.786744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_abbrev_dict(jsonfile):\n",
    "    curr_id = jsonfile.split('.')[0]\n",
    "    with open(os.path.join(JSON_FEATURE_DIR, jsonfile)) as f:\n",
    "        raw_json = json.load(f)\n",
    "        raw_text = list(map(lambda x: ' '.join(x.values()), raw_json))\n",
    "        raw_text = ' '.join(raw_text)\n",
    "        raw_text = ' '.join(raw_text.split())\n",
    "    sentences = '\\n'.join(sent_tokenize(raw_text))\n",
    "    return curr_id, extract_abbreviation_definition_pairs(doc_text=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "crazy-insert",
   "metadata": {
    "_cell_guid": "e8d7f302-bf4c-4af8-a91b-be755906bac1",
    "_uuid": "ede75ffe-481e-4347-9fd9-fd7a62f3d39e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:42.847921Z",
     "iopub.status.busy": "2021-06-26T01:17:42.847277Z",
     "iopub.status.idle": "2021-06-26T01:17:43.258436Z",
     "shell.execute_reply": "2021-06-26T01:17:43.257931Z",
     "shell.execute_reply.started": "2021-06-26T01:16:07.788096Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.431573,
     "end_time": "2021-06-26T01:17:43.258565",
     "exception": false,
     "start_time": "2021-06-26T01:17:42.826992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 ms, sys: 59 ms, total: 70.5 ms\n",
      "Wall time: 405 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the document text search across the test set using multiprocessing\n",
    "with mp.Pool(12) as pooler:\n",
    "    list_dicts = list(pooler.map(get_abbrev_dict, os.listdir(JSON_FEATURE_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-yield",
   "metadata": {
    "_cell_guid": "b51c6c5f-b1bf-478d-8ec5-9e94eaa935e0",
    "_uuid": "140875b3-07b4-4d25-ac49-055e79b63ea0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:17:43.300350Z",
     "iopub.status.busy": "2021-06-26T01:17:43.299778Z",
     "iopub.status.idle": "2021-06-26T01:18:00.661982Z",
     "shell.execute_reply": "2021-06-26T01:18:00.661543Z",
     "shell.execute_reply.started": "2021-06-26T01:16:08.503600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 17.385283,
     "end_time": "2021-06-26T01:18:00.662109",
     "exception": false,
     "start_time": "2021-06-26T01:17:43.276826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# candidate labels: 69\n",
      "CPU times: user 5.44 s, sys: 1.53 s, total: 6.97 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate classifier predictions for all labels\n",
    "candidate_labels = list(set(long for _, curr_dict in list_dicts for long in curr_dict.values()))\n",
    "print('# candidate labels: {}'.format(len(candidate_labels)))\n",
    "candidate_preds = classify_labels(candidate_labels)\n",
    "candidate_prob_mapping = dict(zip(candidate_labels, candidate_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "resistant-insulin",
   "metadata": {
    "_cell_guid": "b5277d0e-063c-466e-a742-6fedb0e98858",
    "_uuid": "5e87c793-7ceb-4e7b-9665-e07f590070cb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:00.708362Z",
     "iopub.status.busy": "2021-06-26T01:18:00.707667Z",
     "iopub.status.idle": "2021-06-26T01:18:00.710660Z",
     "shell.execute_reply": "2021-06-26T01:18:00.711111Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.820304Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031121,
     "end_time": "2021-06-26T01:18:00.711260",
     "exception": false,
     "start_time": "2021-06-26T01:18:00.680139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# All associated IDs: 4\n"
     ]
    }
   ],
   "source": [
    "# Filter for labels that meet the classifier prob req\n",
    "long_id_mapping = defaultdict(set) # Mapping of candidate to document ID\n",
    "clean_raw_mapping = dict()  # Mapping of cleaned candidate string to raw candidate string\n",
    "long_short_mapping = dict()  # Mapping of LONG-NAME candidate to its acronym\n",
    "\n",
    "for curr_id, curr_dict in list_dicts:\n",
    "    for short, long in curr_dict.items():\n",
    "        if candidate_prob_mapping[long] > MIN_PROB: # Only accept labels that meet the minimum probabiity\n",
    "            cleaned_form = clean_text(long)\n",
    "            long_id_mapping[cleaned_form].add(curr_id)\n",
    "            clean_raw_mapping[cleaned_form] = long\n",
    "\n",
    "            # Store only acronyms that are longer than 3 characters\n",
    "            if len(short) > 3:\n",
    "                long_short_mapping[cleaned_form] = short\n",
    "\n",
    "print('# All associated IDs: {}'.format(len(set().union(*[x for x in long_id_mapping.values()]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-municipality",
   "metadata": {
    "papermill": {
     "duration": 0.017851,
     "end_time": "2021-06-26T01:18:00.747026",
     "exception": false,
     "start_time": "2021-06-26T01:18:00.729175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Threshold and propagate\n",
    "- Create a set of high frequency labels (high_prob_freq_labels) including the definite training labels\n",
    "- Search through the cleaned texts of all documents for matches and add them as candidates for the given docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "important-glenn",
   "metadata": {
    "_cell_guid": "a9a9dfe5-6a27-4c2b-bff5-062d31e8c075",
    "_uuid": "ce1d4438-0b98-402d-b536-96a8195ed744",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:00.789605Z",
     "iopub.status.busy": "2021-06-26T01:18:00.788989Z",
     "iopub.status.idle": "2021-06-26T01:18:00.792617Z",
     "shell.execute_reply": "2021-06-26T01:18:00.793231Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.830275Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028425,
     "end_time": "2021-06-26T01:18:00.793656",
     "exception": false,
     "start_time": "2021-06-26T01:18:00.765231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# high prob/high freq labels: 0\n",
      "# of associated IDs: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "high_prob_freq_labels = [k for k, v in long_id_mapping.items() if len(v) > HIGH_FREQ]\n",
    "print('# high prob/high freq labels: {}'.format(len(high_prob_freq_labels)))\n",
    "print('# of associated IDs: {}'.format(len(set().union(*[v for k, v in long_id_mapping.items() if k in high_prob_freq_labels]))))\n",
    "print(high_prob_freq_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dominant-extra",
   "metadata": {
    "_cell_guid": "4643ba62-0eeb-4b19-ba57-e40698448813",
    "_uuid": "9cdafc12-8830-4420-9a5c-674f7a00937b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:00.839465Z",
     "iopub.status.busy": "2021-06-26T01:18:00.838896Z",
     "iopub.status.idle": "2021-06-26T01:18:00.975522Z",
     "shell.execute_reply": "2021-06-26T01:18:00.976388Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.846936Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.163556,
     "end_time": "2021-06-26T01:18:00.976611",
     "exception": false,
     "start_time": "2021-06-26T01:18:00.813055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# combined w/ training labels: 130\n"
     ]
    }
   ],
   "source": [
    "# Throw in external training labels\n",
    "def_labels = set(pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')['cleaned_label'].drop_duplicates())\n",
    "high_prob_freq_labels = set(high_prob_freq_labels).union(def_labels)\n",
    "print('# combined w/ training labels: {}'.format(len(high_prob_freq_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "seasonal-polymer",
   "metadata": {
    "_cell_guid": "db923545-a642-4ba9-9540-d508025215dd",
    "_uuid": "fa4be2d1-8301-4ef8-aa37-a8c70ef68f89",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:01.022778Z",
     "iopub.status.busy": "2021-06-26T01:18:01.022146Z",
     "iopub.status.idle": "2021-06-26T01:18:01.068556Z",
     "shell.execute_reply": "2021-06-26T01:18:01.069169Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.902167Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072499,
     "end_time": "2021-06-26T01:18:01.069374",
     "exception": false,
     "start_time": "2021-06-26T01:18:00.996875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.2 ms, sys: 1.36 ms, total: 34.5 ms\n",
      "Wall time: 41.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Code to generate both raw text and cleaned text for given test json file\n",
    "def get_text_for_jsonid(json_file):\n",
    "    file_id = json_file.split('.')[0]\n",
    "    with open(os.path.join(JSON_FEATURE_DIR, json_file)) as f:\n",
    "        raw_json = json.load(f)\n",
    "        raw_text = list(map(lambda x: ' '.join(x.values()), raw_json))\n",
    "        raw_text = ' '.join(raw_text)\n",
    "        raw_text = ' '.join(raw_text.split())\n",
    "    return file_id, raw_text, clean_text(raw_text)\n",
    "\n",
    "id_text_tuple = list(map(get_text_for_jsonid, os.listdir(JSON_FEATURE_DIR)))\n",
    "\n",
    "id_to_raw_text = {x[0]:x[1] for x in id_text_tuple}\n",
    "id_to_clean_text = {x[0]:x[2] for x in id_text_tuple}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mineral-agency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:01.139214Z",
     "iopub.status.busy": "2021-06-26T01:18:01.137460Z",
     "iopub.status.idle": "2021-06-26T01:18:01.141804Z",
     "shell.execute_reply": "2021-06-26T01:18:01.141152Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.946609Z"
    },
    "papermill": {
     "duration": 0.052807,
     "end_time": "2021-06-26T01:18:01.141978",
     "exception": false,
     "start_time": "2021-06-26T01:18:01.089171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of associated IDs: 4\n"
     ]
    }
   ],
   "source": [
    "# Perform the search of docs\n",
    "for curr_label in high_prob_freq_labels:\n",
    "    for curr_id in id_to_clean_text:\n",
    "        if curr_label in id_to_clean_text[curr_id]:\n",
    "            long_id_mapping[curr_label].add(curr_id)\n",
    "            \n",
    "print('# of associated IDs: {}'.format(len(set().union(*[v for k, v in long_id_mapping.items() if k in high_prob_freq_labels]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "swiss-meter",
   "metadata": {
    "_cell_guid": "300a9647-c2ba-453d-808c-400de11d11b4",
    "_uuid": "1feb3335-216e-458d-90d9-529de4a8a540",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:01.187242Z",
     "iopub.status.busy": "2021-06-26T01:18:01.186418Z",
     "iopub.status.idle": "2021-06-26T01:18:01.189313Z",
     "shell.execute_reply": "2021-06-26T01:18:01.188908Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.974851Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027016,
     "end_time": "2021-06-26T01:18:01.189444",
     "exception": false,
     "start_time": "2021-06-26T01:18:01.162428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate ID to prediction mappings\n",
    "id_to_pred_mapping = defaultdict(list)\n",
    "for curr_label in long_id_mapping:\n",
    "    for curr_id in long_id_mapping[curr_label]:\n",
    "        id_to_pred_mapping[curr_id].append(curr_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-characterization",
   "metadata": {
    "papermill": {
     "duration": 0.031909,
     "end_time": "2021-06-26T01:18:01.254041",
     "exception": false,
     "start_time": "2021-06-26T01:18:01.222132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Collate at the ID level\n",
    "For each candidate dataset of a given document :-\n",
    "1. Remove if too similar to a definite training label (as assessed by FuzzyWuzzy string match algo)\n",
    "2. Accept if \n",
    "\n",
    "    a. Meets HIGH_FREQ document frequency threshold OR \n",
    "    \n",
    "    b. Regex matches ([A-Z][a-z]+ )+(Study|Survey)$ OR \n",
    "    \n",
    "    c. Regex matches (Study|Survey) of\n",
    "    \n",
    "3. If the candidate is accepted, accept its acronym if it's present in the RAW document text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noted-cream",
   "metadata": {
    "_cell_guid": "7db3e847-2fd3-48ba-a68a-e3a974bdc883",
    "_uuid": "2d228256-79e7-4b49-a2a0-ab870c18f30d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:01.337959Z",
     "iopub.status.busy": "2021-06-26T01:18:01.336851Z",
     "iopub.status.idle": "2021-06-26T01:18:01.341936Z",
     "shell.execute_reply": "2021-06-26T01:18:01.342685Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.981284Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055378,
     "end_time": "2021-06-26T01:18:01.342897",
     "exception": false,
     "start_time": "2021-06-26T01:18:01.287519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for curr_id, curr_pred_list in id_to_pred_mapping.items():\n",
    "    # Sort in following descending priority (a definite training label, doc frequency, length of string)\n",
    "    curr_pred_list = sorted(curr_pred_list,\n",
    "                            key=lambda x:(x in def_labels,len(long_id_mapping[x]), 1./len(x)), reverse=True)\n",
    "    sieved_pred_list = []\n",
    "    for curr_pred in curr_pred_list:\n",
    "        match_found = False\n",
    "        for other_pred in sieved_pred_list:\n",
    "            # Check if a candidate is too similar to a definite training label prediction\n",
    "            if fuzz.token_set_ratio(curr_pred, other_pred) > MATCHING_THRESHOLD and curr_pred not in def_labels and other_pred in def_labels:\n",
    "                match_found = True\n",
    "                break\n",
    "                \n",
    "        if not match_found and (len(long_id_mapping[curr_pred]) > HIGH_FREQ or curr_pred in def_labels\n",
    "                                or re.search('([A-Z][a-z]+ )+(Study|Survey)$', clean_raw_mapping[curr_pred])\n",
    "                                or re.search('(Study|Survey) of', clean_raw_mapping[curr_pred])):\n",
    "            sieved_pred_list.append(curr_pred)\n",
    "            \n",
    "            # Add acronym as prediction if present in raw document text\n",
    "            if curr_pred in long_short_mapping and re.search(r' {} '.format(long_short_mapping[curr_pred]),\n",
    "                                                             id_to_raw_text[curr_id]):\n",
    "                sieved_pred_list.append((clean_text(long_short_mapping[curr_pred])))\n",
    "\n",
    "    id_to_pred_mapping[curr_id] = set(sieved_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comfortable-sacramento",
   "metadata": {
    "_cell_guid": "a385b9f4-80c7-47ca-aff4-391bcc8cb518",
    "_uuid": "03742c4b-8b59-4b1d-902d-75050d00998d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:01.414854Z",
     "iopub.status.busy": "2021-06-26T01:18:01.413844Z",
     "iopub.status.idle": "2021-06-26T01:18:01.419978Z",
     "shell.execute_reply": "2021-06-26T01:18:01.421315Z",
     "shell.execute_reply.started": "2021-06-26T01:16:12.996443Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048311,
     "end_time": "2021-06-26T01:18:01.421526",
     "exception": false,
     "start_time": "2021-06-26T01:18:01.373215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dummy labels for #0 missing IDs\n"
     ]
    }
   ],
   "source": [
    "# Add dummy labels for missing IDs (if any)\n",
    "all_ids = set(list(map(lambda x: x.split('.')[0], os.listdir(JSON_FEATURE_DIR))))\n",
    "pred_ids = set(id_to_pred_mapping)\n",
    "missing_ids = all_ids - pred_ids\n",
    "\n",
    "for curr_missing_id in missing_ids:\n",
    "    id_to_pred_mapping[curr_missing_id].append('')\n",
    "\n",
    "print('Added dummy labels for #{} missing IDs'.format(len(missing_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smoking-johnston",
   "metadata": {
    "_cell_guid": "ea41954f-6255-4a5d-a55d-e2aebb033a92",
    "_uuid": "7cb98a29-030d-4db3-9a5a-7d17effb1607",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-06-26T01:18:01.495350Z",
     "iopub.status.busy": "2021-06-26T01:18:01.494306Z",
     "iopub.status.idle": "2021-06-26T01:18:01.708185Z",
     "shell.execute_reply": "2021-06-26T01:18:01.709315Z",
     "shell.execute_reply.started": "2021-06-26T01:16:13.011143Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.256369,
     "end_time": "2021-06-26T01:18:01.709524",
     "exception": false,
     "start_time": "2021-06-26T01:18:01.453155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(id_to_pred_mapping.items())\n",
    "pred_df.columns = ['Id', 'PredictionString']\n",
    "pred_df['PredictionString'] = pred_df['PredictionString'].apply(lambda x: '|'.join(sorted(x)))\n",
    "pred_df = pred_df.sort_values(['Id', 'PredictionString'])\n",
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.82919,
   "end_time": "2021-06-26T01:18:03.701889",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-26T01:17:30.872699",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
